{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generate Multi-Reversal ABC Multi-Timestep Task Sequence**\n",
    "\n",
    "This notebook generates multi-reversal ABC task sequences with multi-timestep trial structure:\n",
    "\n",
    "**Trial Structure:**\n",
    "- **Stimulus window**: Multiple timesteps showing the stimulus\n",
    "- **Reward availability window**: Multiple timesteps where reward can be obtained\n",
    "- **Outcome state**: Determined by action during reward window\n",
    "- **ITI**: Random timesteps (between min_iti and max_iti) showing no stimulus\n",
    "\n",
    "**Task Rules:**\n",
    "- Stimulus A: Rewarded in even phases (0, 2, 4, ...), never in odd phases (1, 3, 5, ...)\n",
    "- Stimulus B: Never rewarded in even phases, rewarded in odd phases\n",
    "- Stimulus C: Random reward (50% probability, doesn't reverse)\n",
    "\n",
    "Sequences are saved as pickle files for use with Gymnasium environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Define Stimuli and Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define stimulus identities and reward values\n",
    "stimuli = {\"A\": 0, \"B\": 1, \"C\": 2}\n",
    "rewards = {\"no_reward\": 0, \"reward\": 1}\n",
    "\n",
    "def generate_random_reward(reward1, reward2, prob):\n",
    "    \"\"\"Generate random reward with given probability.\"\"\"\n",
    "    if np.random.rand(1) < prob:\n",
    "        return reward1\n",
    "    else:\n",
    "        return reward2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Set Task Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Multi-reversal task parameters\n",
    "# Specify number of trials for each reversal phase\n",
    "# A and B reverse: A rewarded -> B rewarded -> A rewarded -> B rewarded -> ...\n",
    "# C always has random 50% reward\n",
    "phase_trials = [2000, 2000, 2000, 2000]  # 4 phases: A rewarded, B rewarded, A rewarded, B rewarded\n",
    "\n",
    "# Trial structure parameters\n",
    "stim_window = 5      # Number of timesteps for stimulus presentation\n",
    "reward_window = 3    # Number of timesteps for reward availability\n",
    "min_iti = 10         # Minimum ITI timesteps\n",
    "max_iti = 20         # Maximum ITI timesteps\n",
    "\n",
    "# Random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(f\"Task parameters:\")\n",
    "print(f\"  Number of phases: {len(phase_trials)}\")\n",
    "for i, num_trials in enumerate(phase_trials):\n",
    "    phase_type = \"A rewarded\" if i % 2 == 0 else \"B rewarded\"\n",
    "    print(f\"  Phase {i} ({phase_type}): {num_trials} trials\")\n",
    "print(f\"  Stimulus window: {stim_window} timesteps\")\n",
    "print(f\"  Reward window: {reward_window} timesteps\")\n",
    "print(f\"  ITI range: {min_iti}-{max_iti} timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Generate Trial-Level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate trial-level data (stimulus and reward availability)\n",
    "trial_data = {\n",
    "    \"stimuli\": [],\n",
    "    \"rewards\": [],\n",
    "    \"masks\": {\"reversal\": []}\n",
    "}\n",
    "\n",
    "# Generate trials for each phase\n",
    "# Phase 0: A rewarded, B not, C random 50%\n",
    "# Phase 1: B rewarded, A not, C random 50%\n",
    "# Phase 2: A rewarded, B not, C random 50%\n",
    "# Phase 3: B rewarded, A not, C random 50%\n",
    "# ... and so on\n",
    "\n",
    "for phase_idx, num_trials in enumerate(phase_trials):\n",
    "    # Determine which stimulus is rewarded in this phase\n",
    "    # Even phases (0, 2, 4, ...): A rewarded\n",
    "    # Odd phases (1, 3, 5, ...): B rewarded\n",
    "    a_rewarded = (phase_idx % 2 == 0)\n",
    "    \n",
    "    for _ in range(num_trials):\n",
    "        stim = np.random.choice([stimuli[\"A\"], stimuli[\"B\"], stimuli[\"C\"]], p=[1/3, 1/3, 1/3])\n",
    "        \n",
    "        if stim == stimuli[\"A\"]:\n",
    "            reward = rewards[\"reward\"] if a_rewarded else rewards[\"no_reward\"]\n",
    "        elif stim == stimuli[\"B\"]:\n",
    "            reward = rewards[\"reward\"] if not a_rewarded else rewards[\"no_reward\"]\n",
    "        else:  # C\n",
    "            reward = generate_random_reward(rewards[\"reward\"], rewards[\"no_reward\"], prob=0.5)\n",
    "        \n",
    "        trial_data[\"stimuli\"].append(stim)\n",
    "        trial_data[\"rewards\"].append(reward)\n",
    "        trial_data[\"masks\"][\"reversal\"].append(phase_idx)\n",
    "\n",
    "print(f\"Generated trial-level data:\")\n",
    "print(f\"  Total trials: {len(trial_data['stimuli'])}\")\n",
    "for i, num_trials in enumerate(phase_trials):\n",
    "    phase_type = \"A rewarded\" if i % 2 == 0 else \"B rewarded\"\n",
    "    print(f\"  Phase {i} ({phase_type}): {num_trials} trials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Expand to Timestep-Level Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Now expand to timestep-level sequence\n",
    "# States: A=0, B=1, C=2, reward_unknown=3, unrewarded=4, rewarded=5, ITI=6\n",
    "state_map = {\"A\": 0, \"B\": 1, \"C\": 2, \"reward_unknown\": 3, \"unrewarded\": 4, \"rewarded\": 5, \"ITI\": 6}\n",
    "state_sequence = []\n",
    "reward_sequence = []\n",
    "trial_structure = []  # Track which timesteps belong to which trial and phase\n",
    "\n",
    "trial_idx = 0\n",
    "timestep = 0\n",
    "\n",
    "for stim, reward_avail, reversal_phase in zip(\n",
    "    trial_data[\"stimuli\"],\n",
    "    trial_data[\"rewards\"],\n",
    "    trial_data[\"masks\"][\"reversal\"]\n",
    "):\n",
    "    trial_start_timestep = timestep\n",
    "    \n",
    "    # Stimulus window: show stimulus\n",
    "    stim_timesteps = []\n",
    "    for _ in range(stim_window):\n",
    "        state_sequence.append(stim)  # Stimulus state\n",
    "        reward_sequence.append(0.0)  # No reward during stimulus window\n",
    "        stim_timesteps.append(timestep)\n",
    "        timestep += 1\n",
    "    \n",
    "    # Reward availability window: start in \"reward_unknown\" outcome state\n",
    "    # The environment will transition to rewarded/unrewarded based on action\n",
    "    reward_timesteps = []\n",
    "    for _ in range(reward_window):\n",
    "        state_sequence.append(state_map[\"reward_unknown\"])  # Outcome state, reward unknown\n",
    "        reward_sequence.append(1.0 if reward_avail == rewards[\"reward\"] else 0.0)\n",
    "        reward_timesteps.append(timestep)\n",
    "        timestep += 1\n",
    "    \n",
    "    # ITI: random length\n",
    "    iti_length = np.random.randint(min_iti, max_iti + 1)\n",
    "    iti_timesteps = []\n",
    "    for _ in range(iti_length):\n",
    "        state_sequence.append(state_map[\"ITI\"])\n",
    "        reward_sequence.append(0.0)\n",
    "        iti_timesteps.append(timestep)\n",
    "        timestep += 1\n",
    "    \n",
    "    trial_structure.append({\n",
    "        \"trial_idx\": trial_idx,\n",
    "        \"stimulus\": stim,\n",
    "        \"reward_available\": reward_avail == rewards[\"reward\"],\n",
    "        \"reversal_phase\": reversal_phase,\n",
    "        \"trial_start\": trial_start_timestep,\n",
    "        \"stim_window\": stim_timesteps,\n",
    "        \"reward_window\": reward_timesteps,  # These are the \"reward_unknown\" outcome states\n",
    "        \"iti_window\": iti_timesteps,\n",
    "        \"trial_end\": timestep - 1\n",
    "    })\n",
    "    \n",
    "    trial_idx += 1\n",
    "\n",
    "print(f\"Expanded to timestep-level sequence:\")\n",
    "print(f\"  Total timesteps: {len(state_sequence)}\")\n",
    "print(f\"  Average trial length: {len(state_sequence) / len(trial_structure):.1f} timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Convert to One-Hot Encoding (7D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert to one-hot encoding\n",
    "# States: A=0, B=1, C=2, reward_unknown=3, unrewarded=4, rewarded=5, ITI=6\n",
    "# We'll use 7D encoding: [A, B, C, reward_unknown, unrewarded, rewarded, ITI]\n",
    "state_sequence_ohe_7d = np.zeros((len(state_sequence), 7), dtype=np.float32)\n",
    "for i, state_idx in enumerate(state_sequence):\n",
    "    if 0 <= state_idx < 7:\n",
    "        state_sequence_ohe_7d[i, state_idx] = 1.0\n",
    "\n",
    "print(f\"Converted to one-hot encoding:\")\n",
    "print(f\"  State sequence shape: {state_sequence_ohe_7d.shape}\")\n",
    "print(f\"  Reward sequence length: {len(reward_sequence)}\")\n",
    "print(f\"  Number of trials: {len(trial_structure)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Calculate Phase Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate phase boundaries\n",
    "phase_boundaries = {\"reversal_points\": []}\n",
    "cumulative_trials = 0\n",
    "\n",
    "for phase_idx, num_trials in enumerate(phase_trials):\n",
    "    if phase_idx == 0:\n",
    "        phase_start_timestep = 0\n",
    "    else:\n",
    "        # Start of this phase is end of previous phase\n",
    "        phase_start_timestep = trial_structure[cumulative_trials][\"trial_start\"]\n",
    "        phase_boundaries[\"reversal_points\"].append(phase_start_timestep)\n",
    "    \n",
    "    # End of this phase\n",
    "    phase_end_trial_idx = cumulative_trials + num_trials - 1\n",
    "    phase_end_timestep = trial_structure[phase_end_trial_idx][\"trial_end\"] + 1\n",
    "    \n",
    "    phase_boundaries[f\"phase_{phase_idx}\"] = {\n",
    "        \"start\": phase_start_timestep,\n",
    "        \"end\": phase_end_timestep\n",
    "    }\n",
    "    \n",
    "    cumulative_trials += num_trials\n",
    "\n",
    "print(f\"Phase boundaries:\")\n",
    "for i in range(len(phase_trials)):\n",
    "    phase_type = \"A rewarded\" if i % 2 == 0 else \"B rewarded\"\n",
    "    print(f\"  Phase {i} ({phase_type}): timesteps {phase_boundaries[f'phase_{i}']['start']} to {phase_boundaries[f'phase_{i}']['end']}\")\n",
    "print(f\"  Reversal points: {phase_boundaries['reversal_points']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Prepare Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare data dictionary\n",
    "data = {\n",
    "    \"state_sequence_ohe\": state_sequence_ohe_7d,\n",
    "    \"reward_sequence\": np.array(reward_sequence, dtype=np.float32),\n",
    "    \"sequence\": {\n",
    "        \"stimuli\": trial_data[\"stimuli\"],\n",
    "        \"rewards\": trial_data[\"rewards\"],\n",
    "        \"masks\": trial_data[\"masks\"]\n",
    "    },\n",
    "    \"phase_boundaries\": phase_boundaries,\n",
    "    \"trial_structure\": trial_structure,\n",
    "    \"state_map\": {\"A\": 0, \"B\": 1, \"C\": 2, \"reward_unknown\": 3, \"unrewarded\": 4, \"rewarded\": 5, \"ITI\": 6},\n",
    "    \"trial_params\": {\n",
    "        \"stim_window\": stim_window,\n",
    "        \"reward_window\": reward_window,\n",
    "        \"min_iti\": min_iti,\n",
    "        \"max_iti\": max_iti\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Data dictionary prepared:\")\n",
    "print(f\"  Keys: {list(data.keys())}\")\n",
    "print(f\"  State sequence shape: {data['state_sequence_ohe'].shape}\")\n",
    "print(f\"  Reward sequence length: {len(data['reward_sequence'])}\")\n",
    "print(f\"  Trial structure length: {len(data['trial_structure'])}\")\n",
    "print(f\"  Number of phases: {len([k for k in data['phase_boundaries'].keys() if k.startswith('phase_')])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save to pickle file\n",
    "output_path = Path(\"/Users/pmccarthy/Documents/cogNN/task_data/reversal_abc_multitimestep_multi.pkl\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, \"wb\") as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "print(f\"\\nGenerated reversal_abc_multitimestep_multi task:\")\n",
    "print(f\"  Total timesteps: {len(data['state_sequence_ohe'])}\")\n",
    "print(f\"  Number of phases: {len([k for k in data['phase_boundaries'].keys() if k.startswith('phase_')])}\")\n",
    "print(f\"  Reversal points: {data['phase_boundaries']['reversal_points']}\")\n",
    "print(f\"  Saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}